# Install pre-built version of llama.cpp

| Install via | Windows | Mac | Linux |
|-------------|---------|-----|-------|
| Winget      | ✅      |      |      |
| Homebrew    |         | ✅   | ✅   |
| MacPorts    |         | ✅   |      |
| Nix         |         | ✅   | ✅   |

## Winget (Windows)

```sh
winget install llama.cpp
```

The package is automatically updated with new `llama.cpp` releases. More info: https://github.com/ggml-org/llama.cpp/issues/8188

## Homebrew (Mac and Linux)

```sh
brew install llama.cpp
```

The formula is automatically updated with new `llama.cpp` releases. More info: https://github.com/ggml-org/llama.cpp/discussions/7668

## MacPorts (Mac)

```sh
sudo port install llama.cpp
```

See also: https://ports.macports.org/port/llama.cpp/details/

## Nix (Mac and Linux)

```sh
nix profile install nixpkgs#llama-cpp
```

For flake enabled installs.

Or

```sh
nix-env --file '<nixpkgs>' --install --attr llama-cpp
```

For non-flake enabled installs.

This expression is automatically updated within the [nixpkgs repo](https://github.com/NixOS/nixpkgs/blob/nixos-24.05/pkgs/by-name/ll/llama-cpp/package.nix#L164).

How to install models from huggingface: 
# download + run a public GGUF straight from Hugging Face (no token needed)
llama-cli -hf TheBloke/Mistral-7B-Instruct-v0.2-GGUF:Q4_K_M -cnv
powershell
Copy code
# smaller alt
llama-cli -hf TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF:Q5_K_M -cnv
powershell
Copy code
# if you prefer to download the file first, then run it locally
python -m pip install -U "huggingface_hub[cli]"
hf download TheBloke/Mistral-7B-Instruct-v0.2-GGUF --include "mistral-7b-instruct-v0.2.Q4_K_M.gguf" --local-dir .\models\mistral7b
llama-cli -m .\models\mistral7b\mistral-7b-instruct-v0.2.Q4_K_M.gguf -cnv